{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This first cell is added by Kaggle by default and contains some useful info/setup to be able to open the data files for the competition. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-10T18:12:18.417972Z","iopub.execute_input":"2022-11-10T18:12:18.419339Z","iopub.status.idle":"2022-11-10T18:12:18.457794Z","shell.execute_reply.started":"2022-11-10T18:12:18.419148Z","shell.execute_reply":"2022-11-10T18:12:18.456426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is where we open the dataset for the competition. This can be confusing at first, which is why Kaggle prints out the path for us above. In the top right of the window the data files for this competition are also listed, so that is how we know to use `train.csv`. ","metadata":{}},{"cell_type":"code","source":"input_data_path = '../input/small-talk-intent-classification-data/Small_talk_Intent.csv'\ndf = pd.read_csv(input_data_path)\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:12:24.863243Z","iopub.execute_input":"2022-11-10T18:12:24.863740Z","iopub.status.idle":"2022-11-10T18:12:24.903745Z","shell.execute_reply.started":"2022-11-10T18:12:24.863703Z","shell.execute_reply":"2022-11-10T18:12:24.902728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the data early on to avoid data leakage is always a good idea. You can choose a different holdout percentage with the parameters `test_size` (common or typical values are 15% or 20% of your data, but can vary depending on how much data you have). ","metadata":{}},{"cell_type":"code","source":"# split data into train and validation sets: df_train and df_val\nfrom sklearn.model_selection import train_test_split\n\nX = df['Utterances'].copy()\ny = df['Intent'].copy()\n\nX_train_raw, X_val_raw, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\nX_train_raw.head() + \"     \" + y_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:25:06.090204Z","iopub.execute_input":"2022-11-10T18:25:06.090684Z","iopub.status.idle":"2022-11-10T18:25:06.105318Z","shell.execute_reply.started":"2022-11-10T18:25:06.090647Z","shell.execute_reply":"2022-11-10T18:25:06.103878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next is the vectorization of the data. Note that the tokenization is happening inside the `fit_transform` method as well. Parameters to the `TfidfVectorizer()` constructor can also change how the vectorization is done (e.g. limit vocabularly to __x__ most frequently occuring words, remove stopwords or not, etc.). There is lots there, so checking out the documentation for [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) is not a bad idea. ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n#import seaborn as sns\ntfidf_vectorizer = TfidfVectorizer()\nX_train = tfidf_vectorizer.fit_transform(X_train_raw).toarray()\n\n# an alternative is to use term frequency:\n#from sklearn.feature_extraction.text import CountVectorizer\n#one_hot_vectorizer = CountVectorizer(binary=True)\n#X_train = one_hot_vectorizer.fit_transform(X_train_raw))\n\nprint(f\"X_train.shape = {X_train.shape}\")\ntype(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:25:11.632036Z","iopub.execute_input":"2022-11-10T18:25:11.632464Z","iopub.status.idle":"2022-11-10T18:25:11.663454Z","shell.execute_reply.started":"2022-11-10T18:25:11.632432Z","shell.execute_reply":"2022-11-10T18:25:11.661660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, it's time to fit a model to the data. This is the main part of the assignment is to use a model __other than RandomForest__. Fortunately, this should not be too difficult of a chance to this notebook since Scikit Learn has many, many types of classification models, and they are easily interchangeable. Besides the RandomForestClassifier, there are:\n* [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n* [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n* [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n* [ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n* [HistGradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html)\n* [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n* [Naive Bayes Multinomial Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n\nand even more options. Feel free to choose another one. ","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n#from sklearn.ensemble import AdaBoostClassifier\n#from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n\n\nmodel = MultinomialNB()\n#AdaBoostClassifier(n_estimators=150, random_state=0)\n#model = GaussianNB()\nmodel = model.fit(X_train, y_train)\n\npredictions_train = model.predict(X_train)\n\n#disp = ConfusionMatrixDisplay(confusion_matrix(y_train, predictions_train), display_labels=['negative', 'neutral', 'positive'])\n#disp.plot()\nprint(f\"accuracy (on X_train): {accuracy_score(y_train, predictions_train):.4f}\")","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:26:12.116995Z","iopub.execute_input":"2022-11-10T18:26:12.117494Z","iopub.status.idle":"2022-11-10T18:26:12.178658Z","shell.execute_reply.started":"2022-11-10T18:26:12.117455Z","shell.execute_reply":"2022-11-10T18:26:12.175662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val = tfidf_vectorizer.transform(X_val_raw).toarray()\nprint(f\"X_val.shape = {X_val.shape}\")\ntype(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:26:23.751875Z","iopub.execute_input":"2022-11-10T18:26:23.752476Z","iopub.status.idle":"2022-11-10T18:26:23.764694Z","shell.execute_reply.started":"2022-11-10T18:26:23.752442Z","shell.execute_reply":"2022-11-10T18:26:23.763340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_val = model.predict(X_val)\n#disp = ConfusionMatrixDisplay(confusion_matrix(y_val, predictions_val), display_labels=['negative', 'neutral', 'positive'])\n#disp.plot()\nprint(f\"accuracy (on X_train): {accuracy_score(y_val, predictions_val):.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:26:37.732397Z","iopub.execute_input":"2022-11-10T18:26:37.733582Z","iopub.status.idle":"2022-11-10T18:26:37.746631Z","shell.execute_reply.started":"2022-11-10T18:26:37.733539Z","shell.execute_reply":"2022-11-10T18:26:37.745059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we did previously, let's now check to see if there is any hyperparameter tuning that can be done to further improve the model performance. We saw that for Random Forest there will not be much difference here, but hyperparameter choices for other types of the models can cause performance to vary much more. ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import log_loss\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\ntune_model = True # can change this to False once you've chosen a hyperparam value and before Saving your notebook with Kaggle\nintents = df['Intent'].unique()\n# A function to create and fit a RF with a specific number of trees\ndef tuneModel(hyperparam_value):\n    rf_model = HistGradientBoostingClassifier(max_iter=hyperparam_value, random_state=5)\n    #rf_model = RandomForestClassifier(min_samples_split=hyperparam_value, random_state=1)\n    rf_model.fit(X_train, y_train)\n    y_train_pred_prob = rf_model.predict_proba(X_train)\n    y_train_pred = rf_model.predict(X_train)\n    y_val_pred_prob = rf_model.predict_proba(X_val)\n    y_val_pred = rf_model.predict(X_val)\n    train_loss = log_loss(y_train, y_train_pred_prob, labels=intents)\n    train_acc = accuracy_score(y_train, y_train_pred)\n    val_loss = log_loss(y_val, y_val_pred_prob, labels=intents)\n    val_acc = accuracy_score(y_val, y_val_pred)\n    return (train_loss, val_loss, train_acc, val_acc)\n\n# Possible values for the hyperparameter are in the range of 5 to 150 (by 50)\nhyp_param_vals = list(range(39,43,1)) # good values for n_estimators\nhyp_param_vals = [2,3] + list(range(5, 50, 10)) # good values for min_samples_split\nmetrics = []\n\nif tune_model:\n    for hp in hyp_param_vals:\n        metrics.append(tuneModel(hp))","metadata":{"execution":{"iopub.status.busy":"2022-11-10T18:31:17.843386Z","iopub.execute_input":"2022-11-10T18:31:17.843882Z","iopub.status.idle":"2022-11-10T18:34:32.135872Z","shell.execute_reply.started":"2022-11-10T18:31:17.843835Z","shell.execute_reply":"2022-11-10T18:34:32.134146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the results of the model performance for each hyperparameter value we looked at","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nif tune_model:\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n\n    ax1.set_xticks(hyp_param_vals)\n    ax1.set(xlabel=\"n_estimators\", ylabel=\"loss (lower is better)\")\n    ax1.plot(hyp_param_vals, [metric[1] for metric in metrics], '--ro') # validation loss\n    ax1.plot(hyp_param_vals, [metric[0] for metric in metrics], '--bo') # training loss\n    ax1.legend([\"Validation Loss\", \"Train Loss\"], loc=1)\n\n    ax2.set_xticks(hyp_param_vals)\n    ax2.set(xlabel=\"n_estimators\", ylabel=\"accuracy (higher is better)\")\n    ax2.plot(hyp_param_vals, [metric[3] for metric in metrics], '--ro') # validation accuracy\n    ax2.plot(hyp_param_vals, [metric[2] for metric in metrics], '--bo') # training accuracy\n    ax2.legend([\"Validation Accuracy\", \"Train Accuracy\"], loc=4)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T18:46:43.861261Z","iopub.execute_input":"2022-09-26T18:46:43.861665Z","iopub.status.idle":"2022-09-26T18:46:43.869972Z","shell.execute_reply.started":"2022-09-26T18:46:43.861626Z","shell.execute_reply":"2022-09-26T18:46:43.86919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To be certain, let's look at the table of values and see which hyperparameter value has the lowest.","metadata":{}},{"cell_type":"code","source":"# a simple matrix with first row containing hyperparam values, second row containing validation loss, third row containing validation accuracy\n# (this could be presented in an even nicer format using a pandas dataframe if you like)\nif tune_model:\n    tuning_results = np.array([hyp_param_vals, [round(metric[1],2) for metric in metrics], [round(metric[3],2) for metric in metrics]])\n    print(tuning_results)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T18:46:49.460468Z","iopub.execute_input":"2022-09-26T18:46:49.461242Z","iopub.status.idle":"2022-09-26T18:46:49.466934Z","shell.execute_reply.started":"2022-09-26T18:46:49.461209Z","shell.execute_reply":"2022-09-26T18:46:49.465846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once you're satisified with your model, it's time to make predictions for the test dataset and submit those to the Kaggle competition. You'll first need to load the `test.csv` data file (which, of course, does not have the labels in it).  \n\nNote that you'll likely need to do some hyperparameter tuning depending on the model that you choose. Tuning means training/fitting your model for many different values of the hyperparameter and then choosing the value that resulted in the best performance (i.e. lowest loss). See our [notebook C from class (nb_C_airline_tweets_take2.ipynb)](https://github.com/sgeinitz/cs39aa_notebooks/), at the very bottom of the notebook, for an example of how to do this. ","metadata":{}},{"cell_type":"code","source":"test_data_file = 'test.csv'\ndf_test = pd.read_csv(input_data_path + test_data_file)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-26T18:46:54.194803Z","iopub.execute_input":"2022-09-26T18:46:54.195236Z","iopub.status.idle":"2022-09-26T18:46:54.242819Z","shell.execute_reply.started":"2022-09-26T18:46:54.195203Z","shell.execute_reply":"2022-09-26T18:46:54.241712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll now need to go through the same process of tokenizing and vectorizing the tweets, and then putting them through the model. We need to be careful to not mix up the order of these observations. Specifically, the `id` field is used by Kaggle to know which observation is which (when comparing to the labels that Kaggle is hiding from us). We will be okay as long as we don't shuffle or subset this dataset. ","metadata":{}},{"cell_type":"code","source":"X_test = tfidf_vectorizer.transform(df_test['text']).toarray()\nprint(f\"X_test.shape = {X_test.shape}\")\ntype(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T18:46:58.10454Z","iopub.execute_input":"2022-09-26T18:46:58.10499Z","iopub.status.idle":"2022-09-26T18:46:58.437306Z","shell.execute_reply.started":"2022-09-26T18:46:58.10494Z","shell.execute_reply":"2022-09-26T18:46:58.436021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now make the predictions and peek at the first 10. ","metadata":{}},{"cell_type":"markdown","source":"****I Used the hyperparameters 39 the best loss and accracy****","metadata":{}},{"cell_type":"code","source":"# refit the model with the best hyperparameter value you found\nmodel =  AdaBoostClassifier(n_estimators=100, random_state=0)\nmodel = model.fit(X_train, y_train)\n\n# this make predictions for the test set\npredictions_test = model.predict(X_test)\npredictions_test[:10]","metadata":{"execution":{"iopub.status.busy":"2022-09-26T18:47:39.422998Z","iopub.execute_input":"2022-09-26T18:47:39.424013Z","iopub.status.idle":"2022-09-26T18:50:14.835449Z","shell.execute_reply.started":"2022-09-26T18:47:39.423964Z","shell.execute_reply":"2022-09-26T18:50:14.834192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now append the predictions to the `df_test` data frame as a new column and peek at some of those to see if the predictions look decent. ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df_test['predictions'] = predictions_test\npd.set_option(\"display.max_colwidth\", 240)\ndf_test.head(n=10)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T18:50:39.322879Z","iopub.execute_input":"2022-09-26T18:50:39.32334Z","iopub.status.idle":"2022-09-26T18:50:39.337375Z","shell.execute_reply.started":"2022-09-26T18:50:39.323298Z","shell.execute_reply":"2022-09-26T18:50:39.336328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A submission on Kaggle should have only two fields (or columns). These are a) the `id` and b) the predicted `sentiment`. Those are the exact column names that should be used. The next cell creates a pandas data frame with those two columns, and renames the second column accordingly. ","metadata":{}},{"cell_type":"code","source":"df_submission = df_test[['id','predictions']]\ndf_submission.columns = ['id', 'sentiment']\ndf_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-26T18:50:43.189812Z","iopub.execute_input":"2022-09-26T18:50:43.190254Z","iopub.status.idle":"2022-09-26T18:50:43.210664Z","shell.execute_reply.started":"2022-09-26T18:50:43.190222Z","shell.execute_reply":"2022-09-26T18:50:43.20945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The final step is simply to write this data frame with the test predictions to a csv file. ","metadata":{}},{"cell_type":"code","source":"df_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T18:50:47.881617Z","iopub.execute_input":"2022-09-26T18:50:47.882399Z","iopub.status.idle":"2022-09-26T18:50:47.897863Z","shell.execute_reply.started":"2022-09-26T18:50:47.882342Z","shell.execute_reply":"2022-09-26T18:50:47.896636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After you're certain that this notebook runs correctly, you'll then click on the __Submit__ button on the right side of this window. ","metadata":{}}]}